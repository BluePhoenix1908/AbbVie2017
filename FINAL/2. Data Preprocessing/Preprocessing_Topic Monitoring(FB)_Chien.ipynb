{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing for Topic Monitoring(Facebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "from langdetect import detect\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('maxent_treebank_pos_tagger')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import wordpunct_tokenize\n",
    "from IPython.display import Image\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>created_time.x</th>\n",
       "      <th>id.x</th>\n",
       "      <th>message.x</th>\n",
       "      <th>id.y</th>\n",
       "      <th>message.y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ankylosing Spondylitis</td>\n",
       "      <td>2012-01-01T04:35:04+0000</td>\n",
       "      <td>209801168268_10150460899518269</td>\n",
       "      <td>Can all of the pain and symptoms of ASAP be in...</td>\n",
       "      <td>10150460899518269_20302273</td>\n",
       "      <td>maybe you need to discuss this in great detail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ankylosing Spondylitis</td>\n",
       "      <td>2012-01-01T04:35:04+0000</td>\n",
       "      <td>209801168268_10150460899518269</td>\n",
       "      <td>Can all of the pain and symptoms of ASAP be in...</td>\n",
       "      <td>10150460899518269_20316800</td>\n",
       "      <td>Mine is narrowed and is very painful- to the p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ankylosing Spondylitis</td>\n",
       "      <td>2012-01-01T04:35:04+0000</td>\n",
       "      <td>209801168268_10150460899518269</td>\n",
       "      <td>Can all of the pain and symptoms of ASAP be in...</td>\n",
       "      <td>10150460899518269_20311875</td>\n",
       "      <td>Michelle,I wouldn't do the PT if it is killing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      key            created_time.x  \\\n",
       "0  Ankylosing Spondylitis  2012-01-01T04:35:04+0000   \n",
       "1  Ankylosing Spondylitis  2012-01-01T04:35:04+0000   \n",
       "2  Ankylosing Spondylitis  2012-01-01T04:35:04+0000   \n",
       "\n",
       "                             id.x  \\\n",
       "0  209801168268_10150460899518269   \n",
       "1  209801168268_10150460899518269   \n",
       "2  209801168268_10150460899518269   \n",
       "\n",
       "                                           message.x  \\\n",
       "0  Can all of the pain and symptoms of ASAP be in...   \n",
       "1  Can all of the pain and symptoms of ASAP be in...   \n",
       "2  Can all of the pain and symptoms of ASAP be in...   \n",
       "\n",
       "                         id.y  \\\n",
       "0  10150460899518269_20302273   \n",
       "1  10150460899518269_20316800   \n",
       "2  10150460899518269_20311875   \n",
       "\n",
       "                                           message.y  \n",
       "0  maybe you need to discuss this in great detail...  \n",
       "1  Mine is narrowed and is very painful- to the p...  \n",
       "2  Michelle,I wouldn't do the PT if it is killing...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112171\n"
     ]
    }
   ],
   "source": [
    "### Load the Crawled Facebook Dataset\n",
    "# Remove duplicates, NA, sorted time\n",
    "disease = pd.read_csv('Final_utf16.csv', encoding = 'utf-16LE', sep=',',\n",
    "                         dtype={\"key\": object, \"id.x\": object,\"like_count.x\": float, \"from_id.x\":float,\n",
    "                                \"from_name.x\":object, \"message.x\":object, \"created_time.x\":object, \"type\":object,\n",
    "                                \"link\":object, \"story\":object, \"comments_count.x\":float,\"shares_count\":float,\n",
    "                                \"love_count\":float, \"haha_count\":float, \"wow_count\":float, \"sad_count\": float,\n",
    "                                \"angry_count\":float, \"join_id\":object, \"from_id.y\":float, \"from_name.y\":object,\n",
    "                                \"message.y\":object, \"created_time.y\":object, \"likes_count.y\":float, \n",
    "                                \"comments_count.y\": float, \"id.y\":object})\n",
    "df = pd.DataFrame(disease, columns=['key', 'created_time.x', 'id.x','message.x' , 'id.y', 'message.y'])\n",
    "df.columns = ['key', 'created_time.x', 'id.x','message.x' , 'id.y', 'message.y']\n",
    "rm_duplicates = df.drop_duplicates(subset=['message.x', 'message.y'])\n",
    "dtime = rm_duplicates.sort_values(['created_time.x'])\n",
    "dtime.index = range(len(dtime))\n",
    "dlang = dtime\n",
    "dlang = dlang[dlang['key']!='johnson & johnson']\n",
    "dlang = dlang[dlang['key']!='johnson&johnson']\n",
    "dlang.index = range(len(dlang))\n",
    "display(dlang.head(3))\n",
    "print(len(dlang))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Detect the text language by majority vote\n",
    "def calculate_languages_ratios(text):\n",
    "    languages_ratios = {}\n",
    "    tokens = wordpunct_tokenize(text)\n",
    "    words = [word.lower() for word in tokens]\n",
    "    for language in stopwords.fileids():\n",
    "        stopwords_set = set(stopwords.words(language))\n",
    "        words_set = set(words)\n",
    "        common_elements = words_set.intersection(stopwords_set)\n",
    "        languages_ratios[language] = len(common_elements)\n",
    "    return languages_ratios\n",
    "    \n",
    "def detect_language(text):\n",
    "    ratios = calculate_languages_ratios(text)\n",
    "    most_rated_language = max(ratios, key=ratios.get)\n",
    "    return most_rated_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, preprocessing is implemented into following steps.<br>\n",
    "\n",
    "| Preprocessing Steps| Packages                    | Notes                               |\n",
    "|------------------- |-----------------------------|-------------------------------------|\n",
    "| Language Detection | Self-defined function, nktk |Check the language of each post      |\n",
    "| Remove Stopwords   | nltk.corpus                 |Remove stopwords of detected language|\n",
    "| Remove Url         | Regular expression          |                                     |\n",
    "| Remove Punctuation | string.punctuation          |                                     | \n",
    "| Lemmatizing        | nltk.stem                   |Lemmatize words in Noun and Verb     |\n",
    "| Part of Speech(POS)| nltk.pos_tag                |Preserve Noun, Adverb and Adjective  |\n",
    "| Tokenize           | split                       |Unigram                              |\n",
    "| Remove NA          | pandas                      |                                     |\n",
    "| Drop Duplicates    | pandas                      |                                     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import time\n",
    "import os\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# Create a new csv file to store the result after data preprocessing\n",
    "with open('facebook_preprocessing.csv', 'w', encoding = 'UTF-8', newline = '') as csvfile:\n",
    "    column = [['key', 'created_time.x', 'id.x', 'message.x', 'id.y', 'message.y',\n",
    "               'lang.x', 're_message.x', 'lang.y', 're_message.y']]\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerows(column)\n",
    "\n",
    "# Data preprocessing steps\n",
    "for i in range(len(dlang['message.x'])): \n",
    "    features = []\n",
    "    features.append(dlang['key'][i])\n",
    "    features.append(dlang['created_time.x'][i])\n",
    "    features.append(dlang['id.x'][i])\n",
    "    features.append(dlang['message.x'][i])\n",
    "    features.append(dlang['id.y'][i])\n",
    "    features.append(dlang['message.y'][i])\n",
    "    if(str(dlang['message.x'][i]) == \"nan\"):\n",
    "        features.append('english')\n",
    "        features.append(dlang['message.x'][i])\n",
    "    else:\n",
    "        lang = detect_language(dlang['message.x'][i])\n",
    "        features.append(lang)\n",
    "        stop = set(stopwords.words(lang))\n",
    "        reurl = re.sub(r\"http\\S+\", \"\", str(dlang['message.x'][i]))\n",
    "        tokens = ' '.join(re.findall(r\"[\\w']+\", reurl)).lower().split()\n",
    "        x = [''.join(c for c in s if c not in string.punctuation) for s in tokens]\n",
    "        x = ' '.join(x)\n",
    "        stop_free = \" \".join([i for i in x.lower().split() if i not in stop])\n",
    "        punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "        normalized = \" \".join(lemma.lemmatize(word,pos = 'n') for word in punc_free.split())\n",
    "        normalized = \" \".join(lemma.lemmatize(word,pos = 'v') for word in normalized.split())\n",
    "        word = \" \".join(word for word in normalized.split() if len(word)>3)\n",
    "        postag = nltk.pos_tag(word.split())\n",
    "        irlist = [',','.',':','#',';','CD','WRB','RB','PRP','...',')','(','-','``','@']\n",
    "        poslist = ['NN','NNP','NNS','RB','RBR','RBS','JJ','JJR','JJS']\n",
    "        wordlist = ['co', 'https', 'http','rt','com','amp','fe0f','www','ve','dont',\"i'm\",\"it's\",'isnt','âźă','âąă','âł_','kf4pdwe64k']\n",
    "        adjandn = [word for word,pos in postag if pos in poslist and word not in wordlist and len(word)>3]\n",
    "        stop = set(stopwords.words(lang))\n",
    "        wordlist = [i for i in adjandn if i not in stop]\n",
    "        features.append(' '.join(wordlist))\n",
    "    if(str(dlang['message.y'][i]) == \"nan\"):\n",
    "        features.append('english')\n",
    "        features.append(dlang['message.y'][i])\n",
    "    else:\n",
    "        lang = detect_language(dlang['message.y'][i])\n",
    "        features.append(lang)\n",
    "        stop = set(stopwords.words(lang))\n",
    "        reurl = re.sub(r\"http\\S+\", \"\", str(dlang['message.y'][i]))\n",
    "        tokens = ' '.join(re.findall(r\"[\\w']+\", reurl)).lower().split()\n",
    "        x = [''.join(c for c in s if c not in string.punctuation) for s in tokens]\n",
    "        x = ' '.join(x)\n",
    "        stop_free = \" \".join([i for i in x.lower().split() if i not in stop])\n",
    "        punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "        normalized = \" \".join(lemma.lemmatize(word,pos='n') for word in punc_free.split())\n",
    "        normalized = \" \".join(lemma.lemmatize(word,pos='v') for word in normalized.split())\n",
    "        word = \" \".join(word for word in normalized.split() if len(word)>3)\n",
    "        postag = nltk.pos_tag(word.split())\n",
    "        irlist = [',','.',':','#',';','CD','WRB','RB','PRP','...',')','(','-','``','@']\n",
    "        poslist = ['NN','NNP','NNS','RB','RBR','RBS','JJ','JJR','JJS']\n",
    "        wordlist = ['co', 'https', 'http','rt','com','amp','fe0f','www','ve','dont',\"i'm\",\"it's\",'isnt','âźă','âąă','âł_','kf4pdwe64k']\n",
    "        adjandn = [word for word,pos in postag if pos in poslist and word not in wordlist and len(word)>3]\n",
    "        stop = set(stopwords.words(lang))\n",
    "        wordlist = [i for i in adjandn if i not in stop]\n",
    "        features.append(' '.join(wordlist))\n",
    "    with open('facebook_preprocessing.csv', 'a', encoding='UTF-8', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows([features])\n",
    "        \n",
    "df_postncomment = pd.read_csv('facebook_preprocessing.csv', encoding = 'UTF-8', sep = ',')\n",
    "rm_na = df_postncomment[pd.notnull(df_postncomment['re_message.x'])]\n",
    "rm_na.index = range(len(rm_na))\n",
    "dfinal_fb = pd.DataFrame(\n",
    "    rm_na,\n",
    "    columns = ['key', 'created_time.x', 'id.x', 'message.x', 'id.y', 'message.y', \n",
    "               'lang.x', 're_message.x', 'lang.y', 're_message.y'])\n",
    "dfinal_fb.to_csv(\n",
    "    'final_facebook_preprocessing.csv',\n",
    "    encoding = 'UTF-8',\n",
    "    columns = ['key', 'created_time.x', 'id.x', 'message.x', 'id.y', 'message.y',\n",
    "               'lang.x', 're_message.x', 'lang.y', 're_message.y'])\n",
    "os.remove('facebook_preprocessing.csv')\n",
    "#print(rm_na['re_message.x'][8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>created_time.x</th>\n",
       "      <th>id.x</th>\n",
       "      <th>message.x</th>\n",
       "      <th>id.y</th>\n",
       "      <th>message.y</th>\n",
       "      <th>lang.x</th>\n",
       "      <th>re_message.x</th>\n",
       "      <th>lang.y</th>\n",
       "      <th>re_message.y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ankylosing Spondylitis</td>\n",
       "      <td>2012-01-01T04:35:04+0000</td>\n",
       "      <td>209801168268_10150460899518269</td>\n",
       "      <td>Can all of the pain and symptoms of ASAP be in...</td>\n",
       "      <td>10150460899518269_20302273</td>\n",
       "      <td>maybe you need to discuss this in great detail...</td>\n",
       "      <td>english</td>\n",
       "      <td>pain symptom asap thoracic spine lumbar spine ...</td>\n",
       "      <td>english</td>\n",
       "      <td>maybe great detail sure physical therapy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ankylosing Spondylitis</td>\n",
       "      <td>2012-01-01T04:35:04+0000</td>\n",
       "      <td>209801168268_10150460899518269</td>\n",
       "      <td>Can all of the pain and symptoms of ASAP be in...</td>\n",
       "      <td>10150460899518269_20316800</td>\n",
       "      <td>Mine is narrowed and is very painful- to the p...</td>\n",
       "      <td>english</td>\n",
       "      <td>pain symptom asap thoracic spine lumbar spine ...</td>\n",
       "      <td>english</td>\n",
       "      <td>mine narrow painful point cant even gentle tou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ankylosing Spondylitis</td>\n",
       "      <td>2012-01-01T04:35:04+0000</td>\n",
       "      <td>209801168268_10150460899518269</td>\n",
       "      <td>Can all of the pain and symptoms of ASAP be in...</td>\n",
       "      <td>10150460899518269_20311875</td>\n",
       "      <td>Michelle,I wouldn't do the PT if it is killing...</td>\n",
       "      <td>english</td>\n",
       "      <td>pain symptom asap thoracic spine lumbar spine ...</td>\n",
       "      <td>english</td>\n",
       "      <td>michelle wouldnt kill condition physical activ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      key            created_time.x  \\\n",
       "0  Ankylosing Spondylitis  2012-01-01T04:35:04+0000   \n",
       "1  Ankylosing Spondylitis  2012-01-01T04:35:04+0000   \n",
       "2  Ankylosing Spondylitis  2012-01-01T04:35:04+0000   \n",
       "\n",
       "                             id.x  \\\n",
       "0  209801168268_10150460899518269   \n",
       "1  209801168268_10150460899518269   \n",
       "2  209801168268_10150460899518269   \n",
       "\n",
       "                                           message.x  \\\n",
       "0  Can all of the pain and symptoms of ASAP be in...   \n",
       "1  Can all of the pain and symptoms of ASAP be in...   \n",
       "2  Can all of the pain and symptoms of ASAP be in...   \n",
       "\n",
       "                         id.y  \\\n",
       "0  10150460899518269_20302273   \n",
       "1  10150460899518269_20316800   \n",
       "2  10150460899518269_20311875   \n",
       "\n",
       "                                           message.y   lang.x  \\\n",
       "0  maybe you need to discuss this in great detail...  english   \n",
       "1  Mine is narrowed and is very painful- to the p...  english   \n",
       "2  Michelle,I wouldn't do the PT if it is killing...  english   \n",
       "\n",
       "                                        re_message.x   lang.y  \\\n",
       "0  pain symptom asap thoracic spine lumbar spine ...  english   \n",
       "1  pain symptom asap thoracic spine lumbar spine ...  english   \n",
       "2  pain symptom asap thoracic spine lumbar spine ...  english   \n",
       "\n",
       "                                        re_message.y  \n",
       "0           maybe great detail sure physical therapy  \n",
       "1  mine narrow painful point cant even gentle tou...  \n",
       "2  michelle wouldnt kill condition physical activ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106758\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('final_facebook_preprocessing.csv', encoding = 'UTF-8', sep = ',', index_col = 0)\n",
    "display(test.head(3))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
