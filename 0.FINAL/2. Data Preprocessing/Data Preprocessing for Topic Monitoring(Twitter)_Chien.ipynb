{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing for Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>key</th>\n",
       "      <th>created_time</th>\n",
       "      <th>language</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>836000000000000000</td>\n",
       "      <td>ibrutinib</td>\n",
       "      <td>2017-02-26T22:42:00.000Z</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @szusmani: Acquired mutations associated wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>837000000000000000</td>\n",
       "      <td>psoriasis</td>\n",
       "      <td>2017-03-01T03:16:00.000Z</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @PuckMuckDuck: NJ Muckers - Stop &amp; Thank ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>838000000000000000</td>\n",
       "      <td>psoriasis</td>\n",
       "      <td>2017-03-03T14:00:00.000Z</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @NewingtonComms: thanks to @NParveenG @sean...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id        key              created_time language  \\\n",
       "0  836000000000000000  ibrutinib  2017-02-26T22:42:00.000Z       en   \n",
       "1  837000000000000000  psoriasis  2017-03-01T03:16:00.000Z       en   \n",
       "2  838000000000000000  psoriasis  2017-03-03T14:00:00.000Z       en   \n",
       "\n",
       "                                             message  \n",
       "0  RT @szusmani: Acquired mutations associated wi...  \n",
       "1  RT @PuckMuckDuck: NJ Muckers - Stop & Thank ou...  \n",
       "2  RT @NewingtonComms: thanks to @NParveenG @sean...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153897\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import csv\n",
    "import sys\n",
    "# from nltk import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "import os\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "# os.remove('final_twitter_preprocessing.csv')\n",
    "# os.remove('twitter_preprocessing.csv')\n",
    "\n",
    "### Load the Twitter Dataset\n",
    "# Remove duplicates, NA, preserve only English texts\n",
    "disease = pd.read_csv('TW_Tweet.csv', encoding = 'UTF-8', low_memory = False)\n",
    "df = pd.DataFrame(disease, columns = ['id', 'keyword', 'created', 'language', 'message'])\n",
    "df.columns = ['id', 'key', 'created_time', 'language', 'message']\n",
    "rm_duplicates = df.drop_duplicates(subset = ['key', 'message'])\n",
    "rm_na = rm_duplicates.dropna()\n",
    "dtime = rm_na.sort_values(['created_time'])\n",
    "dtime.index = range(len(dtime))\n",
    "dlang = dtime[dtime['language'] == 'en']\n",
    "dlang = dlang[dlang['key'] != 'johnson & johnson']\n",
    "dlang = dlang[dlang['key'] != 'johnson&johnson']\n",
    "dlang.index = range(len(dlang))\n",
    "display(dlang.head(3))\n",
    "print(len(dlang))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Detection & Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language detection and Translation is implementes by [Yandex API](https://tech.yandex.com/translate/). <br>\n",
    "Here are two function: **get_translation_direction**, **translation**.<br>\n",
    "It is able to detect the language of the text and translate into the language we specify. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Detection:\n",
      "en , Do you know that she is coming?\n",
      "Translation:\n",
      "Sie wissen, dass Sie kommen wird?\n",
      "de , Sie wissen, dass Sie kommen wird?\n"
     ]
    }
   ],
   "source": [
    "### First need to login and get the Yandex API key from https://tech.yandex.com/translate/\n",
    "\n",
    "import json\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# Add your own key here\n",
    "api_key = \"trnsl.1.1.20170328T192339Z.3d51057a7bbbe500.4ae809a6429ea2565ff027216b5233208c1c4f90\"\n",
    "\n",
    "# Detect the language of text\n",
    "def get_translation_direction(api_key,text):\n",
    "    url = \"https://translate.yandex.net/api/v1.5/tr.json/detect?\"\n",
    "    url = url + \"key=\" + api_key\n",
    "    if(text != \"\"):\n",
    "        url = url+\"&text=\"+text\n",
    "    r = requests.get(url)\n",
    "    return (r.json()['lang'])\n",
    "    \n",
    "# Translate the text into English\n",
    "def translation(api_key,text,lang):\n",
    "    url = \"https://translate.yandex.net/api/v1.5/tr.json/translate?\"\n",
    "    url = url + \"key=\" + api_key\n",
    "    if(text != \"\"):\n",
    "        url = url + \"&text=\" + text\n",
    "    if(lang != \"\"):\n",
    "        url = url + \"&lang=\" + lang\n",
    "    r = requests.get(url)\n",
    "    print(''.join(r.json()['text']))\n",
    "    return(''.join(r.json()['text']))\n",
    "    \n",
    "# Add the text you want to detect and the language you want to translate\n",
    "# For lang, you can check here to see the code of language you want to translate https://tech.yandex.com/translate/doc/dg/concepts/api-overview-docpage/\n",
    "# Below is an example for language translation process\n",
    "text = \"Do you know that she is coming?\"\n",
    "lang = \"de\"\n",
    "print(\"Language Detection:\")\n",
    "print(get_translation_direction(api_key, text), ',', text)\n",
    "print(\"Translation:\")\n",
    "print(lang, ',', translation(api_key, text, lang))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this section, preprocessing is implemented into following steps.<br>\n",
    "\n",
    "| Preprocessing Steps| Packages           | Notes                             |\n",
    "|------------------- |--------------------|-----------------------------------|\n",
    "| Remove Url         | Regular expression |                                   |\n",
    "| Remove Stopwords   | nltk.corpus        |                                   |\n",
    "| Remove Punctuation | string.punctuation |                                   |\n",
    "| Lemmatizing        | nltk.stem          |Lemmatize words in Noun and Verb   |\n",
    "| Part of Speech(POS)| nltk.pos_tag       |Preserve Noun, Adverb and Adjective|\n",
    "| Tokenize           | split              |Unigram                            |\n",
    "| Remove NA          | pandas             |                                   |\n",
    "| Drop Duplicates    | pandas             |                                   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import time\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# Create a new csv file to store the result after data preprocessing\n",
    "with open(\n",
    "        'twitter_preprocessing.csv',\n",
    "        'w',\n",
    "        encoding = 'UTF-8',\n",
    "        newline = '') as csvfile:\n",
    "    column = [[\n",
    "        'id', 'key', 'created_time', 'language', 'message', 're_message'\n",
    "    ]]\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerows(column)\n",
    "    \n",
    "# Data preprocessing steps   \n",
    "for i in range(len(dlang['message'])):\n",
    "    features = []\n",
    "    features.append(str(int(dlang['id'][i])))\n",
    "    features.append(dlang['key'][i])\n",
    "    features.append(dlang['created_time'][i])\n",
    "    features.append(dlang['language'][i])\n",
    "    features.append(dlang['message'][i])\n",
    "    reurl = re.sub(r\"http\\S+\", \"\", str(dlang['message'][i]))\n",
    "    tokens = ' '.join(re.findall(r\"[\\w']+\", reurl)).lower().split()\n",
    "    x = [''.join(c for c in s if c not in string.punctuation) for s in tokens]\n",
    "    x = ' '.join(x)\n",
    "    stop_free = \" \".join([i for i in x.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word,pos = 'n') for word in punc_free.split())\n",
    "    normalized = \" \".join(lemma.lemmatize(word,pos = 'v') for word in normalized.split())\n",
    "    word = \" \".join(word for word in normalized.split() if len(word)>3)\n",
    "    postag = nltk.pos_tag(word.split())\n",
    "    irlist = [',','.',':','#',';','CD','WRB','RB','PRP','...',')','(','-','``','@']\n",
    "    poslist = ['NN','NNP','NNS','RB','RBR','RBS','JJ','JJR','JJS']\n",
    "    wordlist = ['co', 'https', 'http','rt','com','amp','fe0f','www','ve','dont',\"i'm\",\"it's\",'isnt','âźă','âąă','âł_','kf4pdwe64k']\n",
    "    adjandn = [word for word,pos in postag if pos in poslist and word not in wordlist and len(word)>3]\n",
    "    stop = set(stopwords.words('english'))\n",
    "    wordlist = [i for i in adjandn if i not in stop]\n",
    "    features.append(' '.join(wordlist))\n",
    "    with open('twitter_preprocessing.csv', 'a', encoding = 'UTF-8', newline = '') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows([features])\n",
    "df_postncomment = pd.read_csv('twitter_preprocessing.csv', encoding = 'UTF-8', sep = ',')\n",
    "df_rm = df_postncomment.drop_duplicates(subset=['id', 're_message'])\n",
    "rm_english_na = df_rm.dropna()\n",
    "rm_english_na.index = range(len(rm_english_na))\n",
    "dfinal_tw = pd.DataFrame(\n",
    "    rm_english_na,\n",
    "    columns = ['id', 'key', 'created_time', 'language', 'message', 're_message'])\n",
    "dfinal_tw.to_csv(\n",
    "    'final_twitter_preprocessing.csv',\n",
    "    encoding = 'UTF-8',\n",
    "    columns = ['id', 'key', 'created_time', 'language', 'message', 're_message'])\n",
    "os.remove('twitter_preprocessing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>key</th>\n",
       "      <th>created_time</th>\n",
       "      <th>language</th>\n",
       "      <th>message</th>\n",
       "      <th>re_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>836000000000000000</td>\n",
       "      <td>ibrutinib</td>\n",
       "      <td>2017-02-26T22:42:00.000Z</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @szusmani: Acquired mutations associated wi...</td>\n",
       "      <td>szusmani mutation associate ibrutinib resistan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>837000000000000000</td>\n",
       "      <td>psoriasis</td>\n",
       "      <td>2017-03-01T03:16:00.000Z</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @PuckMuckDuck: NJ Muckers - Stop &amp; Thank ou...</td>\n",
       "      <td>puckmuckduck muckers thank national sponsor ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>838000000000000000</td>\n",
       "      <td>psoriasis</td>\n",
       "      <td>2017-03-03T14:00:00.000Z</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @NewingtonComms: thanks to @NParveenG @sean...</td>\n",
       "      <td>newingtoncomms thank nparveeng seananstee burn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id        key              created_time language  \\\n",
       "0  836000000000000000  ibrutinib  2017-02-26T22:42:00.000Z       en   \n",
       "1  837000000000000000  psoriasis  2017-03-01T03:16:00.000Z       en   \n",
       "2  838000000000000000  psoriasis  2017-03-03T14:00:00.000Z       en   \n",
       "\n",
       "                                             message  \\\n",
       "0  RT @szusmani: Acquired mutations associated wi...   \n",
       "1  RT @PuckMuckDuck: NJ Muckers - Stop & Thank ou...   \n",
       "2  RT @NewingtonComms: thanks to @NParveenG @sean...   \n",
       "\n",
       "                                          re_message  \n",
       "0  szusmani mutation associate ibrutinib resistan...  \n",
       "1  puckmuckduck muckers thank national sponsor ab...  \n",
       "2  newingtoncomms thank nparveeng seananstee burn...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153861\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('final_twitter_preprocessing.csv', encoding = 'UTF-8', sep = ',', index_col = 0)\n",
    "display(test.head(3))\n",
    "print(len(test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
