% do not change these two lines (this is a hard requirement
% there is one exception: you might replace oneside by twoside in case you deliver 
% the printed version in the accordant format
\documentclass[11pt,titlepage,oneside,openany]{book}
\usepackage{times}
\usepackage[
    backend=biber,
    style=numeric,
]{biblatex}
 
\addbibresource{./bib/biblopgraphy.bib}

\usepackage[acronym]{glossaries}
\newacronym{api}{API}{Application Programming Interface}
\newacronym{http}{HTTP}{Hypertext Transfer Protocol}
\newacronym{rest}{REST}{Representational State Transfer}
\newacronym{sa}{SA}{Sentiment Analysis}
\newacronym{td}{TD}{Topic Detection}
\newacronym{trd}{TRD}{Trend Detection}
\makeglossaries

%\input{./javascript_preemble.tex}
\input{./json_preemble.tex}
\usepackage{parskip} % no indent on paragraphs
\usepackage{hyperref}

\usepackage{comment}

\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{ntheorem}

% \usepackage{paralist}
\usepackage{tabularx}

% this packaes are useful for nice algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% well, when your work is concerned with definitions, proposition and so on, we suggest this
% feel free to add Corrolary, Theorem or whatever you need
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}


% its always useful to have some shortcuts (some are specific for algorithms
% if you do not like your formating you can change it here (instead of scanning through the whole text)
\renewcommand{\algorithmiccomment}[1]{\ensuremath{\rhd} \textit{#1}}
\def\MYCALL#1#2{{\small\textsc{#1}}(\textup{#2})}
\def\MYSET#1{\scshape{#1}}
\def\MYAND{\textbf{ and }}
\def\MYOR{\textbf{ or }}
\def\MYNOT{\textbf{ not }}
\def\MYTHROW{\textbf{ throw }}
\def\MYBREAK{\textbf{break }}
\def\MYEXCEPT#1{\scshape{#1}}
\def\MYTO{\textbf{ to }}
\def\MYNIL{\textsc{Nil}}
\def\MYUNKNOWN{ unknown }
% simple stuff (not all of this is used in this examples thesis
\def\INT{{\mathcal I}} % interpretation
\def\ONT{{\mathcal O}} % ontology
\def\SEM{{\mathcal S}} % alignment semantic
\def\ALI{{\mathcal A}} % alignment
\def\USE{{\mathcal U}} % set of unsatisfiable entities
\def\CON{{\mathcal C}} % conflict set
\def\DIA{\Delta} % diagnosis
% mups and mips
\def\MUP{{\mathcal M}} % ontology
\def\MIP{{\mathcal M}} % ontology
% distributed and local entities
\newcommand{\cc}[2]{\mathit{#1}\hspace{-1pt} \# \hspace{-1pt} \mathit{#2}}
\newcommand{\cx}[1]{\mathit{#1}}
% complex stuff
\def\MER#1#2#3#4{#1 \cup_{#3}^{#2} #4} % merged ontology
\def\MUPALL#1#2#3#4#5{\textit{MUPS}_{#1}\left(#2, #3, #4, #5\right)} % the set of all mups for some concept
\def\MIPALL#1#2{\textit{MIPS}_{#1}\left(#2\right)} % the set of all mips





\begin{document}

\pagenumbering{roman}
% lets go for the title page, something like this should be okay
\begin{titlepage}
	\vspace*{2cm}
  \begin{center}
   {\Large Topic Monitoring in the Pharmaceutical Industry\\}
   \vspace{2cm} 
   {Master Team Project\\}
   \vspace{2cm}
   {presented by\\
    Hailian Hou (123456789)\\
    Chia-Chien Hung (123456789)\\
    Lu Lifei (123456789)\\
    Olga Pogorelaya (123456789)\\
    Ngoc Nam Trung Nguyen (123456789) \\
    Md. Raziul Hasan Al Tariq (123456789)\\
    Alexander Wei{\ss} (123456789)\\
   }
   \vspace{1cm} 
   {submitted to the\\
    Data and Web Science Group\\
    Prof.\ Dr.\ Heiko Paulheim\\
    University of Mannheim\\} \vspace{2cm}
   {August 2017}
  \end{center}
\end{titlepage} 

% no lets make some add some table of contents
\tableofcontents
\newpage

\listofalgorithms

\listoffigures

\listoftables

\lstlistoflistings

\printglossary[type=\acronymtype,style=long, nonumberlist]

% evntuelly you might add something like this
% \listtheorems{definition}
% \listtheorems{proposition}

\newpage


% okay, start new numbering ... here is where it really starts
\pagenumbering{arabic}

\chapter{Introduction}
\label{cha:intro}

Today in 2017 social media is ubiquitous and is taken for granted in our day-to-day life. Nearly every single shop in the city to large companies own social media accounts to communicate with customers or try to engage new ones. The number of active users on the biggest social media platforms is tremendously high. On Facebook we have around 1.871 million active users as of January 2017. Whatsapp and Facebook messenger share the number of 1.000 million active users. For Twitter in comparison to Facebook, the numbers may seem small but with 317 million active users it belongs to the biggest social media platform of our time. (Chaffey 2017-04-2) More than ever people share their thoughts about current global events like conflicts between different countries, political events, terrorism, rising diseases as well as the opinions to companies and products. 

In the course of the ‘Master Team Project - Topic monitoring in the pharmaceutical industry’ of University of Mannheim for Master Data Science and Master Business informatics students, this work especially focuses on the monitoring of social media for the pharmaceutical context. It is processed in cooperation with AbbVie Inc. located in Ludwigshafen, Germany, a pharmaceutical company focused on both biopharmaceuticals and small molecule drugs. The goal of this project is to find efficient and effective ways to retrieve data from different social media platforms to analyze sentiments, public opinions, emotions towards different events, find suitable topics and detect rising trends in social media. To accomplish this task, this work utilizes different data mining algorithms to detect the requested features in the messages of users. The declared aim is to find a good combination of different of those algorithms to provide reliable results and findings so that especially AbbVie can use and incorporate them into their attempt to monitor social media.

This report describes how to gather data from different social media platforms and what restrictions will apply to them. It focuses on Facebook and Twitter, because this selection covers the most used and the most reactive social media platform today. Furthermore it deals with different machine learning algorithms, explains how they could be applied to different use cases and suggests situation when and how to use them.  


 
\section{Motivation and Goals}
 
With the rapid growth of social media platforms like Twitter and Facebook in the recent years those has become interesting for companies to find out what customer think and talk about their company and products. A lot of people share their thoughts and experience about all manner of things. With Twitter we have one the highest reactive social media platforms of our time. Almost instantly people write messages (tweet) to react accordingly to new events in the world. Due to this fact companies try to effectively monitor Twitter and other platforms to get insights into the current customer situation regarding their product and company reputation. They do not need to organize big surveys among customers to find out these key values. It is sufficient to crawl social media platforms to get user insights and use this data in combination with data mining techniques to extract user sentiments and detect rising trends. This can all be done without the users concerned noticing or being able to influence such intrusion in a high manner. 
This work in particular focuses on the pharmaceutical industry to find out how customers react to different drugs and diseases. We want provide recommendations and workflow to utilize social media platforms to collect data and apply data mining algorithms to extract valuable results. The first goal is to detect the sentiment of users regarding drugs, diseases and different pharmacy companies. This is especially related to our partner AbbVie Inc. . With these result it shall be possible to get an overview about the reputation about AbbVie, its products and its competitors have on social media. Furthermore with the detection of sentiments in disease related texts we have the opportunity to get to know the 
The second goal of this project is the detection of topics and rising trends inside the social media data we gather from the different platforms. This is especially useful to find out what the users talk about and in what they are interested in in specific time ranges. These results can help to plan customer engagement strategies more accurate and related to current trends. 

With this project we want to provide good insights into the structure of social media data and show suitable ways to extract sentiments, topic and trends out of it. Furthermore we want to provide you with some recommendations and hints you may adopt to successfully mine social media data.
 
 

\section{Paper Structure}

In chapter \ref{cha:fundamentals} we discuss the fundamentals, which explains the basic understanding about social media platforms, sentiment analysis, topic detection and trend detection.
		
In chapter, we introduce and explain thoroughly step by step the final output of our project, namely ‘SentiTomo’. The product overview gives a general idea about SentiTomo and how it works and what it can achieve. Later we explain various modules it incorporates, like the data collection module, data preprocessing module, sentiment analysis module, topic detection module, trend detection module and finally the dashboard module.
				
In Chapter 3, we present our recommendations and conclude the report.


In  the appendix, we have enclosed various tables and charts which we have used in this report, along with our bibliography.



\chapter{Fundamentals}
\label{cha:fundamentals}

At first we want to provide an explanation to some fundamental terms which will be used in this report. This should give an overview what the different terms mean and in which context they are used.

\section{Social Media Data}
\label{sec:scoial_media}

The term \textit{social media} refers to all web applications which are built on the technological foundations of the Web 2.0. In comparison to the beginning of the web, Web 2.0 is characterized by a huge improvement of communication channels between users and increased communications between those. Social media platforms heavily rely on this progressive development of user interaction. They offer a place where users can collaborate and interact with each other, share and create content with ease. Unlike traditional static websites, the content of social media platforms is nearly entirely made by the users, which lead to a very reactive and accurate picture of the community who is using the network. These platforms can be divided in different types. The following shows a non-exhaustive list of those. \cite{Rouse2016-09-15}\cite{Rouse2015-01-15}

\begin{itemize}
	\item Forums (Reddit)
	\item Microblogging services (Twitter)
	\item Social networks (Facebook, Google+)
	\item Wikis
	\item Live casting (YouTube, Twitch)\\
	\item Video sharing (YouTube, Twitch)
\end{itemize}

Social media is an integral part of everyone’s life and a world without it is nearly no longer imagineable. For companies it offers a broad variety of possibilities to engage and get to know their customers better. Social media analytics help to support business decisions, mostly based on the customer sentiment, which makes it a great datasource to analyze data. \cite{Rouse2016-09-15}
\par
In this work we will mainly referring to the term social media in the context of two platforms, Twitter and Facebook, as those are building the base for our data gathering process. 


\section{Sentiment Analysis}
\label{sec:sentiment_analysis}

\acrfull{sa} belongs to the broad field of data mining techniques. With the help of Natural Language Processing and text analytics, subjective information are extracted and quantified from texts and messages primarily from social media sources. \acrshort{sa} methods are classification algorithms which are based on labelled data. This data is curated by human annotators who define pre-train classes to different types examples of data. These annotations later then are used to train the different algorithms which quantify the sentiment of texts based on these pre-annotations. The results of those techniques can quantify the sentiment of people's opinions and feelings towards a certain subject of the text. Mostly these are products, companies, other people or current events. Sometimes sentiment analysis is also known as opinion mining or emotion AI.

With the help of \acrlong{sa} companies can detect subjective key values for their organization and implement measures which take advantage of these findings. Some examples of these values are brand reputation along with the popularity of it, the company reputation overall and new product perception. This is essentially useful when new products are introduced and the acceptance of those needs to be measured as fast as possible. \cite{Techopedia2014-01-21}

In this report sentiment analysis is used to determine the opinions of people regarding the pharmaceutical industry. It tries to apply suitable data mining algorithms to quantify the opinions regarding drugs, diseases and companies which are active in this sector.

\section{Topic Detection}
\label{sec:topic_detection}

\acrfull{td} copes with the problem of identifying topics to unseen or unidentified events. It can be applied to a static collection of data ("retrospective detection") or even on real-time ("on-line detection") feed of data from different sources. To detect topics, miscellaneous data mining algorithms can be used. They are all based on using unlabeled data to train them. Unlabeled data means, that it is non curated by human annotators which detect topics beforehand. The algorithms just take raw data and try to cluster them into different groups (topics) based on common attributes among different data objects. \cite{Seo2004}

Topic Detection within this work is primarily used on real-time news. It is first trained on pre-crawled data from social media platforms but later applied to the continuously stream of new data.

\section{Trend Detection}
\label{sec:trend_detection}

\acrfull{trd} techniques try to find out what is suddenly increasing in popularity. This is especially applicable to highly reactive social media platforms. For some types organizations it is tremendously important to know what are trends among the people, especially for news organizations, governments and retailers. 
At most of the time a trend is coupled with a specific topic. If it possible to detect trending topics, organization can recognize what people think is noteworthy and can react to those events appropriately. \cite{Kolb2015-07-16}

\chapter{SentiTomo}
\label{cha:sentitomo}

In this section we want to discuss the different steps we took to build up our database and what algorithms we applied to them in order to fulfil the task of this work.


\section{Overview}
\subsection{Technical Explanation}


As we decided to develop a small application which utilizes our findings to show a way how to integrate them into a real-world-like environment we need to find a suitable programming platform to use for this approach. For our data mining algorithms we used different programming languages. In total we had three which are R, Python and Java. But to let each of them cooperate with each other we had to find a base programming language which “wraps” around them. After a small evaluation we chose Javascript as our language to go. With the rise of Node.js, a framework for writing Javascript code on the server side, it is possible to invoke each of our machine learning languages inside Javascript. We used it to develop a Node.js based server application which can be run on any operating system which supports Node.js. Additionally to the plain server application it serves a front end view which is used to view the results of the machine learning algorithms. The whole application is backed by a MySQL database which saves the results sentiment analysis and topic detection algorithms. Additionally it is used to save raw tweets and their authors for a better evaluation of the results. We chose a SQL based database because it was the easiest one for us to set up and in this use case it does not have any serious disadvantages compared to a NoSQL database.

Once the server is started it constantly uses the Twitter API to retrieve Tweets and their authors which match our keywords in table [keyword table]. These are then saved in the database. Parallel to this procedure two worker threads are running which detect sentiments and topics of tweets. The results of those are also getting saved into the database. They are separated into different tables inside the database provide a clear division between raw data and the result of data mining algorithms. 

A detailed description of the implementation and structure of SentiTomo can be found in our separate sentitomo.md file. This file also includes detailed instruction of how to set up the application.



\section{Data Collection}
\label{sec:datacoll}

The first step we take is to collect data from related social media platforms, in our case Facebook and Twitter. We utilize the \acrfull{api}s of those to retrieve posts, comments (Facebook) and tweets (Twitter) from users. This is done simultaneously for both Facebook and Twitter. We constructed a list of keywords related to pharmaceuticals industry, companies, diseases and products as "anchors" for the crawlers, to collect suitable and appropriate data. These keywords were used for both platforms.

\begin{table}[h]

\begin{center}
\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}>{\scriptsize}>{\scriptsize}c>{\scriptsize}c>{\scriptsize}c}
\hline
Products   & Companies        & Diseases               \\ \hline\hline
adalimumab & abbvie           & ankylosing spondylitis \\
humira     & amgen            & arthritis              \\
enbrel     & johnson\&johnson & hepatitis              \\
ibrutinib  &                  & psoriasis              \\
           &                  & rheumatoid arthritis   \\
           &                  & trilipix               \\ \hline
\end{tabular*}
\caption{Anchor keywords for collecting data}
\label{tab:collkeywords}
\end{center}
\end{table}

\subsection{Facebook API}
\label{subsec:fbapi}

\subsubsection{Overview}
\label{subsub:fboverview}
Like every major web based platform, Facebook offers an \acrfull{api} to programmatically interact with its data. This allows developers to build applications which can utilize the social connection and profile information to make them more involving. Furthermore they have access to public data on Facebook as well as the opportunity to publish posts and messages to the news feed. Officially the API is called Graph API, because it is follows the concept of a “Social Graph” \cite{Facebook2015-02-18}. It consists of three key objects:

\begin{enumerate}
   \item Nodes
   \begin{itemize}
     \item Describe all things that are shown on the Facebook web page: User Pages, Posts, Comments and Images
   \end{itemize}
   \item Edges
   \begin{itemize}
     \item Describe the relation between these things. For example the comments to an Image posted on a Page
   \end{itemize}
   \item Fields
   \begin{itemize}
     \item Contain additional information to nodes. For example the name of pages or the relationship status of users
   \end{itemize}
\end{enumerate}

The Graph API is based on the simple \acrfull{http} and can therefore work with every programming language which implements a \acrshort{http} library. Due to this fact it is very easy to implement the API into any application. It supports common request methods like, GET for retrieving information, POST for uploading data and DELETE for deleting data. With this report we exclusively use GET requests to get data from the Graph, as we do not want modify data on Facebook. 


\subsubsection{Restrictions}
\label{subsub:fbrestrictions}

Facebook offers a highly efficient and very good documented API for getting data from the social network. So it would be actual a very good data source for social media monitoring, but unfortunately there are some restrictions which makes it hard to work. In the following the most serious ones are listed.

\begin{enumerate}
   \item Data scraping terms of service
   \begin{itemize}
     \item Facebook owns a dedicated terms of service document for collecting data from Facebook 		       through automated means. Before any automatic data collection takes place, a written permission has to be obtained by Facebook. Without this permission it is not allowed to scrape any data. Also other restrictions like not selling, or transferring the collected data apply at any given time. So it is absolutely necessary to read through these terms of services and contact Facebook before choosing it as a reliable datasource.~\cite{Facebook2010-04-10}
   \end{itemize}
   \item Streaming API
   \begin{itemize}
     \item Facebook API offers no streaming endpoint, so no real time feed about new content on the platform can be obtained programmatically. It is up to the developer to create processes to constantly crawl for updated data on Facebook with static endpoints.
   \end{itemize}
   \item Rate limits
   \begin{itemize}
     \item The Graph API has some rate limits which will apply if too many requests are sent to it. During this work the rate limit only appeared once while a lot of pages for one keyword were crawled. So we assume that the rate limitation will not apply many times while using the API.~\cite{Facebook2016-06-09} 
   \end{itemize}
   \item No access to public posts
   \begin{itemize}
     \item On April 30th, 2015 made a breaking change to their search API endpoint. By then it was possible to search for public posts on Facebook through \url{/search?type=post&q=foobar}. After this day the endpoint was deprecated and exclusively available for approved companies. From then on, it was not possible to get any public user post. With this change in the API, it lost the most promising endpoint for companies for monitoring the network. Without the opportunity to crawl public posts from users the only available reliable source for getting posts is the page endpoint. With this it is possible to search the pages directory regarding specific keywords and retrieve posts from them. 
In fact, a company could try to apply for a license to get access to the post search endpoint, but it seems that this is only available to a limited set of media publishers.~\cite{Facebook2013-09-06} 
   \end{itemize}
\end{enumerate}



\subsubsection{Workflow}

The restrictions explained in \autoref{subsub:fbrestrictions} lead us to nearly drop Facebook from our workflow, but we tried to find a suitable way to get the most out of the remaining available endpoints.
To overcome the deprecation of the post search endpoint, we tried to get as much information out of the pages search endpoint as we could. This serves the base endpoint for our work. It offers the opportunity to specify keywords which the pages need to match to be returned by the API. We used the keywords from \autoref{tab:collkeywords} to crawl the pages directory. Afterwards it was necessary to restrict the retrieved pages to only those which were in a pharmaceutical context. Then the posts and comments from the pages can be crawled and saved for later work.\par
One major drawback of this approach is that the amount of pages and posts on these pages are very low. Only a few conversation about adverse drug reaction, opinions about drugs and companies take place on them. Furthermore the conversation inside those pages seems to be very limited and biased related to the keyword of the page.
So this approach may not lead to a good overview about opinions on different keywords but for us it was the most suitable way to get data from Facebook.

\subsubsection{The dataset}
\subsubsection{Conclusion}
At first Facebook seemed to be a good and reliable data source for this project. But quickly it became clear that this is not the case. With the restriction of the public post search it is impossible to get all user posts about a keyword. It’s only possible to get posts of pages and the corresponding comments to them. But if often occurs that posts on pages are very biased regarding the keyword of the page. Also a lot of pages in the pharmaceutical context seem to not get updated very often and some seem to be very abandoned. So the breaking point here definitely is the depreciation of the API endpoint for searching posts. This leads to the fact, that Facebook can not be used as a suitable data source for topic monitoring social media. Therefore we dropped the Facebook dataset out of the calculation and algorithms.


\subsection{Twitter API}´
\label{subsec:twapi}

\subsubsection{Overview}
\label{subsub:twoverview}


Like Facebook, Twitter also offers \acrshort{api}s to programmatically access data. They are divided into two different parts. One part are static \acrfull{rest} conform ones which use simple\acrshort{http} requests to create new tweets, read user profile, search for tweets and more.
\par
\noindent The second part are dynamic \acrshort{api}s, better known as the Streaming API. They give applications access to the global real-time stream of Twitter data, without any overhead or by pulling data from the static \acrshort{api} endpoints. In total Twitter offers three different streaming endpoints:

\begin{enumerate}
   \item Public stream: For public Twitter data
   \item User stream: For data associated with only one user
   \item Site stream: Multi user version of user streams
\end{enumerate}

This streaming endpoints differ significantly from the static endpoints. They are keeping a persistent \acrshort{http} connection open between client and server. The server then is streaming tweets as they occur and the client is responsible for processing and storing the result. This resolves into a realtime stream of twitter data which makes any application working with this model reactive and everytime up-to-date.\cite{Twitter2012-05-15}

\subsubsection{Restriction}
\label{subsub:twrestrictions}

Unlike Facebook, Twitter does not have that many restrictions we have to respect in our use case.

\begin{enumerate}
   \item Date restriction in the search API (Static REST API)
   \begin{itemize}
     \item When you want to use the static search endpoint, where you can search for tweets with specific keywords only the latest two weeks from the date of the request are available to search. So it is generally impossible to search for historical tweets through the API. There are some companies like GNIP \cite{Gnip2015-10-20}, which are offering service to get access to historical data. To get access to this data you have to contact those companies to get a custom historical dataset, but this option was not suitable for this project as it would had involve payments and our goal is to avoid that.
   \end{itemize}
   \item Requests per minute (Static REST API)
   \begin{itemize}
     \item The REST endpoints, in contrast to the streaming endpoints are rate limited. So developers have to take care of the amount of requests that are sent to the API. A full overview about the different limits can be found at \url{https://dev.twitter.com/rest/public/rate-limits} \cite{Twitter2016-11-22}
   \end{itemize}
   \item Concurrent streaming APIs (Streaming API)
   \begin{itemize}
     \item Any application which is using any of the different streaming APIs can only open only one stream to Twitter. This have to be kept in the mind if a developer wants to use multiple streaming endpoint. 
   \end{itemize}

\end{enumerate}



\subsubsection{Workflow}

While getting to know the API better we decided to utilize the public streaming \acrshort{api} to get the tweets for specific keywords mentioned in \autoref{tab:collkeywords}. This endpoint accepts a filter value, which defines what Tweets we will receive from the \acrshort{api}. The keywords for that filter can either be separated by a comma, which will work as an \textbf{OR} concatenation or separated by an white space, which will work as an \textbf{AND} concatenation. An example request to the endpoint could look like the following:

\begin{lstlisting}[caption={Twitter streaming endpoint},captionpos=b,language=json,label={lst:twitterStreaming}]
https://stream.twitter.com/1.1/statuses/filter.json?&track=abbvie,humira,adalimumab...
\end{lstlisting}

This leads very convenient way to crawl data from Twitter only for specific topics. Another advantage of using the streaming API is that rate limits will not apply to them. Therefore is possible to receive a arbitrary number of tweets from it without getting rate limited, which is especially useful if peaks of tweets appear at certain events. The only restriction which applies is that it is not possible to open multiple data streams to the stream endpoint at a given time, but in our case this is not necessary as only the public one is used.
Furthermore the response from this endpoint is very comprehensive so it is not necessary to get data from the static endpoints, which will benefit in not going to reach any rate limits. A truncated example response can be seen \autoref{lst:exTwitterAPIRes}.


\begin{lstlisting}[caption={Example Twitter API response},captionpos=b,language=json,label={lst:exTwitterAPIRes}]
{ created_at: 'Mon Aug 21 13:17:14 +0000 2017',
  id: 899621415120494600,
  id_str: '899621415120494597',
  text: 'RT @NRAS_UK: Rheumatoid Arthritis: What You Need to Know. Read article by @Treated_com here: https://t.co/Z036Mzepee https://t.co/bIh4kREAIi',
  source: '<a href="http://twitter.com/#!/download/ipad" rel="nofollow">Twitter for iPad</a>',
  truncated: false,
  in_reply_to_status_id: null,
  in_reply_to_status_id_str: null,
  in_reply_to_user_id: null,
  in_reply_to_user_id_str: null,
  in_reply_to_screen_name: null,
  user: [Object],
  geo: null,
  coordinates: null,
  place: null,
  contributors: null,
  retweeted_status: [Object], // !null if this is a retweet, contains information about original tweet
  is_quote_status: false,
  retweet_count: 0,
  favorite_count: 0,
  entities:
   { hashtags: [],
     urls: [ [Object] ],
     user_mentions: [ [Object], [Object] ],
     symbols: [],
     media: [ [Object] ] },
  extended_entities: { media: [ [Object] ] },
  favorited: false,
  retweeted: false,
  possibly_sensitive: false,
  filter_level: 'low',
  lang: 'en',
  timestamp_ms: '1503321434179' }
\end{lstlisting}



\subsubsection{The dataset}

As of August 20th 2017 we stopped crawling data from the Twitter API. Our base dataset for Twitter then consists of 2 types of objects: Tweets and the authors of those. The most important one of these two is the tweet object with it's message field. The message is the fundamental part which is used to build models for sentiment analysis and topic detection. It contains the the tweet message along with URLs, Emojis and hashtags. The author object was only crawled for the sake of completeness and in case if it will be needed in some circumstances. The definition of the tweet object can be seen in \autoref{lst:tweetObjDef}.

\begin{lstlisting}[caption={Tweet object definition for the database},captionpos=b,language=json,label={lst:tweetObjDef}]
  id: {
        type: Sequelize.STRING,
        primaryKey: true,
    },
    keywordType: {
        type: Sequelize.STRING
    },
    keyword: {
        type: Sequelize.STRING
    },
    created: { // when was the tweet created
        type: Sequelize.DATE
    },
    createdWeek: { 
        type: Sequelize.INTEGER
    },
    toUser: {
        type: Sequelize.STRING
    },
    language: {
        type: Sequelize.STRING
    },
    source: {
        type: Sequelize.STRING
    },
    message: {
        type: Sequelize.STRING
    },
    hashtags: { //hashtags inside the message
        type: Sequelize.STRING
    },
    latitude: { // latitude of the tweet author
        type: Sequelize.STRING
    },
    longitude: { // longitude of the tweet author
        type: Sequelize.STRING
    },
    retweetCount: {
        type: Sequelize.INTEGER
    },
    favorited: {
        type: Sequelize.BOOLEAN
    },
    favoriteCount: {
        type: Sequelize.INTEGER
    },
    isRetweet: { // is the tweet a retweet 
        type: Sequelize.BOOLEAN
    },
    retweeted: { // was this tweet retweeted 
        type: Sequelize.INTEGER
    }
\end{lstlisting}



 
\begin{table}[h]

\begin{center}
\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}>{\scriptsize}>{\scriptsize}c>{\scriptsize}c}
\hline
Type   & Amount   		\\ \hline\hline
Tweets & 250732     		\\
Retweets & 27631     	\\
Answer to tweets & 597  	\\
User & 48555        	\\
 \\ \hline
\end{tabular*}
\caption{Common numbers of the Twitter dataset}
\label{tab:twdataset}
\end{center}
\end{table}

\begin{table}[h]

\begin{center}
\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}>{\scriptsize}>{\scriptsize}c>{\scriptsize}c>{\scriptsize}c}
\hline
Keyword  		& Amount    & Proportion in \%	\\ \hline\
psoriasis & 46070      &  	25.5245		\\
hepatitis c & 35089  &  	19.4406				\\
rheumatoid arthritis & 28940 &   16.0339		\\
amgen & 21404 &  11.8586					\\
abbvie & 14149  &	7.8391					\\
bristol myers & 13355 & 	7.3992				\\
johnson\&johnson & 5641  &	3.1253			\\
arthritis & 3513  &		1.9463				\\
ankylosing spondylitis c & 3473  &	1.9242	\\
humira & 3029  	&	1.6782					\\
ibrutinib & 1851  	&	1.0255				\\
hepatitis & 1662  	&	0.9208				\\
NULL & 764 &  0.4233							\\
enbrel & 669  &	0.3707					\\
adalimumab & 667      &  0.3695				\\
imbruvica & 172 & 0.0953  					\\
trilipix & 45   &  0.0249  				\\ \hline\

\end{tabular*}
\caption{Twitter tweets per keyword}
\label{tab:twdatasettweets}
\end{center}
\end{table}


\subsubsection{Conclusion}
After we had this massive disappointment with the Facebook dataset, Twitter is by far the better source for building up a dataset for data mining. It offers the opportunity to crawl tweets in real-time with no need to access the static endpoints. The \acrshort{api} gives us access to every user tweet matching the keywords, which ensures that at most of the time only appropriate and suitable tweets are crawled. In addition to those advantages the streaming endpoint do not have an rate limitation, which makes it even easier to programmatically crawl tweets. This advantages make sure that Twitter is the primary data source for this project.



\subsection{Technology Stack}
\label{sec:datatechstack}

The following description should emphasize how the tools we used to gather data from social media changed during the progress of this project.

In the first four months of the project we used corresponding R packages to access the data from Facebook and Twitter. For Facebook this was a package called \textit{RFacebook}~\cite{Barbera2017-05-25} and for Twitter the package was called \textit{twitteR}~\cite{Grentry2015-07-29}. The data was crawled manually by executing different R-Scripts which were saving their results into \textit{.csv} files. These files then were uploaded to \textit{Google Drive} to have common database-like location for our data. But with the progress of this work the organization of the different files becomes unmanageable and we decided to save our data in a MySQL database.
Simultaneously to that we switched our crawling stack and implemented it based on Node.js server which is directly connected to the database. With the database we had a structured place for our data which offered a better way to have access to our data than the \textit{.csv} files. Furthermore the \textit{.csv} files were encoding dependent which lead to a lot of issues during the initial crawl of data, the database solved this by not being affected of encodings.
The implementation of the crawling processes was done with the help of different Node packages: \textit{twitter}~\cite{Morris2017-01-13} for retrieving data from the streaming API of Twitter and \textit{fbgraph}~\cite{Oliveira2017-01-19} for Facebook. 

In contrast to the \textit{twitteR} package \textit{twitter} for Node.js is able to crawl the public stream of Twitter data. With that it was able to set up an automatic crawl mechanism which saved suitable tweets, regarding the keywords in \autoref{tab:collkeywords}, into our database. Because of that the manual data collection for Twitter was postponed and only served as a backup mechanism. A truncated (left out database saving) part of the code, which was responsible for the crawling and saving can be found in \autoref{lst:twitterCrawler} 

As listed in \autoref{subsub:fbrestrictions} Facebook does not offer a real-time stream to their data. Therefore the crawling of the Facebook data still had to be done manually by invoking methods from the \textit{fbgraph} package on our server. The result of those manual crawls then was upserted into the database as well. A sample code of this process can be seen in \autoref{lst:facebookCrawler}

The update of our technology stack during the project led to a suitable and reliable way of crawling data from Twitter and Facebook. The centralised data consolidation inside the MySQL database additionally improved the workflow for our machine learning processes.

The final technology stack based on Node.js (JavaScript):

\begin{enumerate}
	\item Node.js server running at least 8 hours a day
	\item Twitter-Crawler for crawling data form the public stream \acrshort{api} \textit{twitter} package~\cite{Morris2017-01-13} (automatic)
	\item Facebook-Crawler for crawling data from the GraphAPI \textit{fbgraph} package~\cite{Oliveira2017-01-19} (manual)
	\item MySQL database for saving tweets and authors (Twitter), pages,posts and comments (Facebook)
\end{enumerate}



\printbibliography


\appendix
\chapter{Program Code / Resources}
\label{cha:appendix-a}


\begin{lstlisting}[caption={TwitterCrawler.track},captionpos=b,language=JavaScript,label={lst:twitterCrawler}]
/**
* @function track 
* @param {String} filters The filters which are used to track tweets from the Twitter API
* @description Starts using the Twitter API with the specified filters. When a new tweet is crawled it upserts the author data
* and inserts the raw tweet data into the database.
* @see {@link module:Connectors~TwitterAuthor}
* @see {@link module:Connectors~Tweet}
* @memberof TwitterCrawler
* @return {void}
*/
track(filters) {
    logger.log('info', 'Start streaming Twitter tweets with filters: ' + filters);
    this.client.stream(
        'statuses/filter', {
            track: filters
        },
        stream => {
            stream.on('data', event => {
                if (event.lang == 'en') {
                	//Save event (tweet) into the database
                }
            });
            stream.on('error', function (error) {
                logger.log('error', error);
                //throw error;
            });
        }
    );
}
\end{lstlisting}



\begin{lstlisting}[caption={FacebookCrawler.searchAndSaveFBPages},captionpos=b,language=JavaScript,label={lst:facebookCrawler}]
 /**
* @function searchAndSaveFBPages
* @param  {String} keyword Keyword for searching Facebook pages
* @description Searches for Facebook pages which match the keyword. It also inserts the found pages and their posts and comments into the database. 
* Only pages with matching categories (.env file config) are saved.
* @see File /server/.env
* @memberof FacebookCrawler
* @returns {void} 
*/
searchAndSaveFBPages(keyword) {
	logger.info("Started Facebook search");

    var pages = new Array();

   	var searchOptions = {
    		q: keyword,
        type: "page",
        fields: "name,id,feed{id,link,message,story,likes,comments{id,message},created_time},category"
   }

   graph.search(searchOptions, async (err, res) => {
   //Iterate over results and save them into the db
   });
}
\end{lstlisting}







\chapter{Further Experimental Results}
\label{cha:appendix-b}

In the following further experimental results are ...


\newpage


\pagestyle{empty}

\begin{comment}
\section*{Ehrenw\"ortliche Erkl\"arung}
Ich versichere, dass ich die beiliegende Master-/Bachelorarbeit ohne Hilfe Dritter
und ohne Benutzung anderer als der angegebenen Quellen und Hilfsmittel
angefertigt und die den benutzten Quellen w\"ortlich oder inhaltlich
entnommenen Stellen als solche kenntlich gemacht habe. Diese Arbeit
hat in gleicher oder \"ahnlicher Form noch keiner Pr\"ufungsbeh\"orde
vorgelegen. Ich bin mir bewusst, dass eine falsche Er- kl\"arung rechtliche Folgen haben
wird.
\\
\\

\noindent
Mannheim, den 31.08.2014 \hspace{4cm} Unterschrift



\begin{table}[h]

\begin{center}
\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}>{\scriptsize}l|>{\scriptsize}c>{\scriptsize}c>{\scriptsize}c|>{\scriptsize}c>{\scriptsize}c>{\scriptsize}c>{\scriptsize}c} 
& \multicolumn{3}{>{\scriptsize}c|}{Baselines} & \multicolumn{4}{>{\scriptsize}c}{Decision Tree} \\\hline
Ontology & M(edian) & G(ood) & E(vil) & results & $\Delta$-M & $\Delta$-G & $\Delta$-E \\\hline\hline
\#301 & 0.825 & 0.877 & 0.877 & 0.855 & +0.030 & -0.022 & -0.022 \\\hline
\#302 & 0.709 & 0.753 & 0.753 & 0.753 & +0.044 & +0.000 & +0.000 \\\hline
\#303 & 0.804 & 0.860 & 0.891 & 0.816 & +0.012 & -0.044 & -0.075 \\\hline
\#304 & 0.940 & 0.961 & 0.961 & 0.967 & +0.027 & +0.006 & +0.006 \\\hline
\bfseries Average & \bfseries 0.820 & \bfseries 0.863 & \bfseries 0.871 & \bfseries 0.848 & \bfseries +0.028 & \bfseries -0.015 & \bfseries -0.023 

\end{tabular*}
\caption[Good vs. Evil]{Comparison between the Good and the Evil}
\label{tab:confonly}
\end{center}
\end{table}



\begin{figure}
	\begin{center}
	\includegraphics[width=5cm]{good.png}
	\caption[Angel]{Child Angel and a white dove.}
	\label{fig:angel}
	\end{center}
\end{figure}
\end{comment}
\end{document}
